{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Reading: Tensor Gradients and Jacobian Products\n",
    "\n",
    "In many cases, we have a scalar loss function, and we need to compute\n",
    "the gradient with respect to some parameters. However, there are cases\n",
    "when the output function is an arbitrary tensor. In this case, PyTorch\n",
    "allows you to compute so-called **Jacobian product**, and not the actual\n",
    "gradient.\n",
    "\n",
    "For a vector function $\\vec{y}=f(\\vec{x})$, where\n",
    "$\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and\n",
    "$\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, a gradient of\n",
    "$\\vec{y}$ with respect to $\\vec{x}$ is given by **Jacobian\n",
    "matrix**:\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "Instead of computing the Jacobian matrix itself, PyTorch allows you to\n",
    "compute **Jacobian Product** $v^T\\cdot J$ for a given input vector\n",
    "$v=(v_1 \\dots v_m)$. This is achieved by calling ``backward`` with\n",
    "$v$ as an argument. The size of $v$ should be the same as\n",
    "the size of the original tensor, with respect to which we want to\n",
    "compute the product:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp+1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{inp.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{inp.grad}\")\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{inp.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵对矩阵的微分\n",
    "\n",
    "torch.autograd.functional.jacobian(func, inputs, create_graph=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7091, 0.0941, 0.8965],\n",
      "        [0.6672, 0.3535, 0.8873]])\n",
      "tensor([5.5816, 5.8015])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[2.0321, 1.0987, 2.4509],\n",
       "         [0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000],\n",
       "         [1.9489, 1.4240, 2.4286]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd.functional import jacobian\n",
    "\n",
    "def exp_reducer(x):\n",
    "    return x.exp().sum(dim=1)\n",
    "\n",
    "inputs = torch.rand(2, 3)\n",
    "print(inputs)\n",
    "\n",
    "y = exp_reducer(inputs)\n",
    "print(y)\n",
    "\n",
    "jacobian(exp_reducer, inputs)  # 每个y对应到x每个数的偏微分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5615, -0.5593,  0.0366], requires_grad=True)\n",
      "tensor([-0.0027,  0.6148,  0.0055])\n",
      "tensor([-0.5641,  0.0555,  0.0422], grad_fn=<AddBackward0>)\n",
      "tensor([1., 1., 1.])\n",
      "--------------------another way\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 矩阵对标量的微分\n",
    "\n",
    "a = torch.randn(3)\n",
    "def func(x):\n",
    "    return x + a\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = func(x)\n",
    "print(x)\n",
    "print(a)\n",
    "print(y)\n",
    "\n",
    "y.backward(torch.ones_like(y))\n",
    "print(x.grad)\n",
    "\n",
    "print(\"----\"*5 + \"another way\")\n",
    "\n",
    "torch.ones_like(y) @ jacobian(func, x)  # v^T * jacobian(func, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5047,  0.0639, -0.8845],\n",
      "        [ 0.4956,  0.0256,  0.7591]], requires_grad=True)\n",
      "tensor([[ 0.5781, -1.0928],\n",
      "        [ 0.5879,  2.8409],\n",
      "        [ 0.5730, -0.0609]], requires_grad=True)\n",
      "tensor([-0.5147,  3.4289,  0.5121])\n",
      "tensor([-0.5147,  3.4289,  0.5121])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5147,  3.4289,  0.5121],\n",
       "        [-0.5147,  3.4289,  0.5121]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2, 3, requires_grad=True)\n",
    "b = torch.randn(3, 2, requires_grad=True)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "y = a @ b\n",
    "\n",
    "# 计算 dL / da\n",
    "y.backward(torch.ones_like(y))  # backward 只能计算标量，不能计算矢量\n",
    "\n",
    "def func(a):\n",
    "    return a @ b\n",
    "\n",
    "# 矩阵求导，需要把一个变量控制住，计算另一个\n",
    "print(torch.ones_like(func(a[0])) @ jacobian(func, a[0]))\n",
    "print(torch.ones_like(func(a[1])) @ jacobian(func, a[1]))\n",
    "a.grad\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
