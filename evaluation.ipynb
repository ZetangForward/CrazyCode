{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Version 1.0.0, Evaluate the visual results\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "def cal_text2image_similarity(text: List[str], image: Image, processor: CLIPProcessor, model: CLIPModel):\n",
    "    image = Image.open(image)\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    return logits_per_image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"/zecheng/svg_model_hub/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"/zecheng/svg_model_hub/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24.6737]], grad_fn=<TBackward0>)\n"
     ]
    }
   ],
   "source": [
    "text = [\"head\"]\n",
    "image = \"/workspace/zecheng/SUWA/output.png\"\n",
    "res = cal_text2image_similarity(text, image, processor, model)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
