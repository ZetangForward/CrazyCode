{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Version 1.0.0, Evaluate the visual results\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, CLIPVisionModel\n",
    "\n",
    "def cal_text2image_similarity(text: List[str], image: Image, processor: CLIPProcessor, model: CLIPModel):\n",
    "    image = Image.open(image)\n",
    "    inputs = processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
    "    return logits_per_image\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"/zecheng/svg_model_hub/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"/zecheng/svg_model_hub/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"head\"]\n",
    "image = \"/workspace/zecheng/SUWA/output.png\"\n",
    "res = cal_text2image_similarity(text, image, processor, model)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.054302683518472e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from pytorch_fid import fid_score\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoProcessor, CLIPVisionModel\n",
    "import numpy as np\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"/zecheng/svg_model_hub/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"/zecheng/svg_model_hub/clip-vit-base-patch32\")\n",
    "\n",
    "image1, image2 = \"/workspace/zecheng/SUWA/output.png\", \"/workspace/zecheng/SUWA/output.png\"\n",
    "\n",
    "# def cal_fid_score(image1: Image, image2: Image, clip_model: CLIPModel, device: str):\n",
    "#     \"\"\"\n",
    "#     Calculate the FID score\n",
    "\n",
    "image1, image2 = Image.open(image1), Image.open(image2)\n",
    "\n",
    "inputs1 = processor(images=image1, return_tensors=\"pt\")\n",
    "inputs2 = processor(images=image2, return_tensors=\"pt\")\n",
    "\n",
    "outputs1, outputs2 = model(**inputs1), model(**inputs2)\n",
    "features1, features2 = outputs1.last_hidden_state, outputs2.last_hidden_state\n",
    "\n",
    "# Calculate FID score  \n",
    "# fid = fid_score.calculate_fid_given_paths([features1, features2], dims=2048, device='cuda', batch_size=features1.size(0)) \n",
    "from torch import FloatTensor\n",
    "\n",
    "def __to_numpy_array(feats) -> np.ndarray:\n",
    "    if isinstance(feats, list):\n",
    "        # flatten list of batch-processed features\n",
    "        if isinstance(feats[0], FloatTensor):\n",
    "            feats = [x.detach().cpu().numpy() for x in feats]\n",
    "    else:\n",
    "        feats = feats.detach().cpu().numpy()\n",
    "    return np.concatenate(feats)\n",
    "\n",
    "features1 = __to_numpy_array(features1)\n",
    "features2 = __to_numpy_array(features2)\n",
    "\n",
    "m1 = features1.mean(axis=0)\n",
    "s1 = np.cov(features1, rowvar=False)  \n",
    "  \n",
    "m2 = features2.mean(axis=0)  \n",
    "s2 = np.cov(features2, rowvar=False)  \n",
    "\n",
    "eps = 1e-6  \n",
    "s1 += eps * np.eye(s1.shape[0])  \n",
    "s2 += eps * np.eye(s2.shape[0]) \n",
    "\n",
    "fid_value = fid_score.calculate_frechet_distance(m1, s1, m2, s2)  \n",
    "\n",
    "\n",
    "print(fid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1, outputs2 = model(**inputs1), model(**inputs2)\n",
    "features1, features2 = outputs1.last_hidden_state, outputs2.last_hidden_state\n",
    "\n",
    "features1\n",
    "\n",
    "features1 = __to_numpy_array(features1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(feats)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m features1 \u001b[39m=\u001b[39m __to_numpy_array(features1)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m features1 \u001b[39m=\u001b[39m __to_numpy_array(features1)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m m1 \u001b[39m=\u001b[39m features1\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m s1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcov(features1, rowvar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)  \n",
      "\u001b[1;32m/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb Cell 4\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         feats \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m feats]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     feats \u001b[39m=\u001b[39m feats\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bzecheng_worker0/workspace/zecheng/SUWA/CrazyCode/evaluation.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(feats)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "from torch import FloatTensor\n",
    "\n",
    "def __to_numpy_array(feats) -> np.ndarray:\n",
    "    if isinstance(feats, list):\n",
    "        # flatten list of batch-processed features\n",
    "        if isinstance(feats[0], FloatTensor):\n",
    "            feats = [x.detach().cpu().numpy() for x in feats]\n",
    "    else:\n",
    "        feats = feats.detach().cpu().numpy()\n",
    "    return np.concatenate(feats)\n",
    "\n",
    "features1 = __to_numpy_array(features1)\n",
    "features1 = __to_numpy_array(features1)\n",
    "\n",
    "m1 = features1.mean(axis=0)\n",
    "s1 = np.cov(features1, rowvar=False)  \n",
    "  \n",
    "m2 = features2.mean(axis=0)  \n",
    "s2 = np.cov(features2, rowvar=False)  \n",
    "  \n",
    "fid_value = fid_score.calculate_frechet_distance(m1, s1, m2, s2)  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
