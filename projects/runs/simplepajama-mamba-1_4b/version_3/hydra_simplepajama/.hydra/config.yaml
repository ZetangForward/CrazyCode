platform:
  name: langchao_suda
  hf_model_path: /public/home/ljt/hf_models
  dataset_path: /public/home/ljt/tzc/data
  exp_path: /public/home/ljt/tzc/ckpt
  result_path: /public/home/ljt/tzc/data/evaluation
JOB_ID: 3
state: train
exp_task: simplepajama
model_name: mamba-1_4b
optimizer:
  optimizer_type: adamw
  lr: 5.0e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  warmup_steps: 0
experiment:
  seed: 27
  model_save_dir: ${exp_task}-${model_name}/version_${JOB_ID}
  save_top_k: 2
  monitor_metric: train_lm_loss
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 0.001
  num_training_steps: 10000
  warmup_steps: 1000
  peak_lr: 0.0002
  last_lr: 1.0e-05
  device_num: 5
  node_num: 1
task:
  dataset:
    data_path: slimpajama-per-source-length-upsample-0.5B
    processed_data_path: slimpajama-per-source-length-upsample-0.5B/processed
    split:
    - train.pkl
    - valid.pkl
    module: custom_dataset.simplepajama
    class_name: SimplepajamaDataset
    max_seq_length: 6000
    nworkers: 12
    train_batch_size: 1
    val_batch_size: 1
    pin_memory: false
    inference_mode: false
    cluster_batch: true
  other_cfgs: null
model:
  model_name_or_path: mamba-1.4b
  ckpt_path: /nvme/hf_models/mamba-1.4b/pytorch_model.bin
  load_model_state_dict: false
  use_position: false
  tokenizer_name_or_path: mamba-1.4b-hf
