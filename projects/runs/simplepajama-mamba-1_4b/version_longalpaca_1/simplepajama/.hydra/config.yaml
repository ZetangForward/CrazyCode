platform:
  name: langchao_suda
  hf_model_path: /public/home/ljt/hf_models
  dataset_path: /public/home/ljt/tzc/data
  exp_path: /public/home/ljt/tzc/ckpt
  result_path: /public/home/ljt/tzc/data/evaluation
mark: longalpaca_1
state: train
exp_task: simplepajama
model_name: mamba-1_4b
optimizer:
  optimizer_type: adamw
  lr: 5.0e-05
lr_scheduler:
  scheduler_type: get_cosine_schedule_with_warmup
  warmup_steps: 0
experiment:
  seed: 27
  model_save_dir: ${exp_task}-${model_name}
  save_top_k: 2
  monitor_metric: train_lm_loss
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.1
  eps: 0.001
  num_training_steps: 10000
  warmup_steps: 1000
  peak_lr: 0.0002
  last_lr: 1.0e-05
  debug: false
  hf_trainer: false
  low_rank_train: false
  device_num: 8
  node_num: 1
task:
  dataset:
    data_path: LongAlpaca-12k/LongAlpaca-12k.json
    processed_data_path: null
    max_seq_length: 7000
    module: custom_dataset.longlora
    class_name: LongLoRA
    nworkers: 4
    train_batch_size: 1
    val_batch_size: 1
    pin_memory: false
    inference_mode: false
    cluster_batch: false
  other_cfgs: null
model:
  model_name_or_path: mamba-1.4b-hf
  tokenizer_name_or_path: mamba-1.4b-hf
  ckpt_path: /nvme/hf_models/mamba-1.4b-hf/pytorch_model.bin
  load_model_state_dict: false
  use_relative_position: false
  use_abs_position: false
  max_position_embeddings: 16384
