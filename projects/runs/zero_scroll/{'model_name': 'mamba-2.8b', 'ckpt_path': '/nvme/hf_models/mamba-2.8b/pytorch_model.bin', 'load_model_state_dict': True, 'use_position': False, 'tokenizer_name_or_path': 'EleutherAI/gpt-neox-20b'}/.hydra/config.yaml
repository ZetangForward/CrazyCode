platform:
  hf_model_path: /nvme/hf_models
  dataset_path: /nvme/zecheng/data
  exp_path: /nvme/zecheng/ckpt
  result_path: /nvme/zecheng/evaluation
exp_task: zero_scroll
state: test
model:
  model_name: mamba-2.8b
  ckpt_path: /nvme/hf_models/mamba-2.8b/pytorch_model.bin
  load_model_state_dict: true
  use_position: false
  tokenizer_name_or_path: EleutherAI/gpt-neox-20b
experiment:
  seed: 27
  results_save_dir: ${exp_task}/${model}/results
  device_num: 1
  node_num: 1
task:
  dataset:
    data_path: ZeroSCROLLS
    processed_data_path: ZeroSCROLLS/all_testing_data.pkl
    ctx_len: 10000
    subsets:
    - gov_report
    - summ_screen_fd
    - qmsum
    - qasper
    - narrative_qa
    - quality
    - musique
    - squality
    - space_digest
    - book_sum_sort
    nworkers: 12
    pin_memory: false
    inference_mode: true
    cluster_batch: false
  other_cfgs:
    max_generation_length: 256
