SAVE_DIR: &SAVE_DIR "/zecheng2/vqllama"
JOB_ID: &JOB_ID 12
EXP_NAME: &EXP_NAME "vqllama_quantizer"

hydra:
  job:
    id: ${JOB_ID}
    name: hydra_${EXP_NAME}
  run:
    dir: ${SAVE_DIR}/${EXP_NAME}/version_${JOB_ID}/${hydra.job.name} 

dataset:
  # train_data_path: "/zecheng2/svg/icon-shop/pkl_data/full_data.pkl"
  train_data_path: "/zecheng2/svg/icon-shop/pkl_data/full_data_snaps/sub_full_data_7.pkl"
  max_path_nums: 512
  min_path_nums: 4
  pad_token_id: 0
  train_batch_size: 128
  val_batch_size: 32
  nworkers: 16
  pin_memory: False
  x_channels: 9
  inference_mode: False
  vocab_size: 200  # max number
  return_all_token_mask: False
  num_bins: 9
  remove_redundant_col: False  # remove the 2nd and 3rd coloumns
  cluster_batch: True

lfq:
  dim: 9
  num_quantizers: 2
  codebook_size: 8192
  

experiment:
  model_save_dir: ${SAVE_DIR}
  exp_name: ${EXP_NAME}
  version: ${JOB_ID}
  beta_1: 0.9
  beta_2: 0.999
  lr: 0.0003
  weight_decay: 0.0
  eps: 0.00000001
  lr_warmup: 100.0
  lr_decay: 10000000000.0
  lr_gamma: 1.0
  lr_scale: 1.0
  lr_use_linear_decay: False
  lr_start_linear_decay: 0
  lr_use_cosine_decay: False
  max_epoch: 150
  device_num: 1
  node_num: 1