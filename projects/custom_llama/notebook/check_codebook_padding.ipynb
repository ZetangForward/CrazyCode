{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查给codebook引入Padding之后是否会影响模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data from /zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl ...\n",
      "begin to sanity check the dataset and conduct pre_process, num of samples: 2000, it will take some time...\n",
      "load config files from /workspace/zecheng/modelzipper/projects/custom_llama/configs/deepspeed/vqvae_config.yaml\n",
      "config loaded successfully!\n",
      "config: namespace(ckpt_path='/zecheng2/vqllama/vqllama_quantizer/version_8/checkpoints/last.ckpt', pad_token_id=70, vqvae=namespace(levels=2, downs_t=[1, 1], strides_t=[2, 2], emb_width=4096, l_bins=8192, l_mu=0.99, spectral=0.0, multispectral=1.0, hvqvae_multipliers=[2, 1, 1], loss_fn='l2', dilation_growth_rate=1, use_nonrelative_specloss=True, use_bottleneck=True, commit=1.0, recon=1.0, linf_k=2048, use_modified_block=False), vqvae_conv_block=namespace(depth=4, width=512, m_conv=1.0, dilation_growth_rate=1, dilation_cycle=None, vqvae_reverse_decoder_dilation=True), dataset=namespace(max_path_nums=512, min_path_nums=4, pad_token_id=0, train_batch_size=128, val_batch_size=32, nworkers=16, pin_memory=False, x_channels=9, inference_mode=False, vocab_size=200, return_all_token_mask=False, num_bins=9, remove_redundant_col=False))\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "import transformers\n",
    "import sys\n",
    "sys.path.append(\"/workspace/zecheng/modelzipper/projects/custom_llama\")\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import Trainer\n",
    "from modelzipper.tutils import *\n",
    "from data.vqllama_dataset import VQDataCollator, VQLLaMAData\n",
    "from models.vqvae import VQVAE\n",
    "from data.svg_data import *\n",
    "import pytorch_lightning as pl\n",
    "from test import postprocess\n",
    "\n",
    "VQVAE_CONFIG_PATH = \"/workspace/zecheng/modelzipper/projects/custom_llama/configs/deepspeed/vqvae_config.yaml\"\n",
    "DATA_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl\"\n",
    "\n",
    "content = auto_read_data(DATA_PATH)\n",
    "dataset = BasicDataset(dataset=content)\n",
    "vqvae_config = load_yaml_config(VQVAE_CONFIG_PATH)\n",
    "\n",
    "block_kwargs = dict(\n",
    "        width=vqvae_config.vqvae_conv_block.width, \n",
    "        depth=vqvae_config.vqvae_conv_block.depth, \n",
    "        m_conv=vqvae_config.vqvae_conv_block.m_conv,\n",
    "        dilation_growth_rate=vqvae_config.vqvae_conv_block.dilation_growth_rate,\n",
    "        dilation_cycle=vqvae_config.vqvae_conv_block.dilation_cycle,\n",
    "        reverse_decoder_dilation=vqvae_config.vqvae_conv_block.vqvae_reverse_decoder_dilation\n",
    "    )\n",
    "\n",
    "class PluginVQVAE(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "vqvae = VQVAE(vqvae_config, multipliers=None, **block_kwargs)\n",
    "plugin_vqvae = PluginVQVAE(vqvae)\n",
    "checkpoint = torch.load(vqvae_config.ckpt_path)  # load vqvae ckpt\n",
    "plugin_vqvae.load_state_dict(checkpoint['state_dict'])\n",
    "plugin_vqvae.eval()\n",
    "plugin_vqvae.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_compress_padding_mask(x):\n",
    "\n",
    "    # 确保长度是偶数，如果是奇数，可以添加一个值以配合压缩逻辑\n",
    "    if len(x) % 2 != 0:\n",
    "        x = torch.cat((x, torch.tensor([False])))\n",
    "\n",
    "    # 压缩mask\n",
    "    # 使用.view(-1, 2)将原始mask分为两列，然后使用.any(dim=1)检查每对是否有任何True值\n",
    "    x = x.view(-1, 2).any(dim=1)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'postprocess' from 'test' (/opt/conda/envs/llama/lib/python3.10/test/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m postprocess\n\u001b[1;32m      4\u001b[0m sample \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m max_seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'postprocess' from 'test' (/opt/conda/envs/llama/lib/python3.10/test/__init__.py)"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]['svg_path']\n",
    "max_seq_len = 512\n",
    "padded_sample = torch.concatenate([sample, torch.zeros(max_seq_len - sample.shape[0], 9)])\n",
    "padding_mask = ~(padded_sample == 0).all(dim=1, keepdim=True).squeeze()\n",
    "compress_padding_mask = cal_compress_padding_mask(padding_mask)\n",
    "svg_token_ids, _ = plugin_vqvae.model.encode(padded_sample.unsqueeze(0), start_level=0, end_level=1)\n",
    "svg_token_ids = svg_token_ids[0]  # 这里是不加padding mask的svg token ids\n",
    "\n",
    "remain_svg_token_ids = svg_token_ids[:, :compress_padding_mask.sum()] # 这里是加入padding mask的svg token ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_padding_mask = cal_compress_padding_mask(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4769, 6022, 2332, 7758, 1569, 6788, 5531, 7700, 6174, 4948, 7938, 4768,\n",
       "         7525, 4729, 7938, 7561, 3135, 4246, 4114, 8102, 3024, 5743, 5396, 5017,\n",
       "         5853, 1010, 3793, 4587, 7054, 3964, 4413, 3545,  507, 2131, 6577, 3549,\n",
       "         3431, 1318, 7670, 4729, 7149, 4534, 2645, 3456, 7525, 5071, 3070, 1404,\n",
       "         3410, 3949, 5163, 2390, 4154, 6460, 7098, 7818, 2706, 5812, 7903, 5782,\n",
       "          530, 2238,  291, 2219, 3717, 3410, 4328, 5163, 6670, 2634, 3785, 6468,\n",
       "         6595, 2494, 5209]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remain_svg_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4769, 6022, 2332, 7758, 1569, 6788, 5531, 7700, 6174, 4948, 7938, 4768,\n",
       "         7525, 4729, 7938, 7561, 3135, 4246, 4114, 8102, 3024, 5743, 5396, 5017,\n",
       "         5853, 1010, 3793, 4587, 7054, 3964, 4413, 3545,  507, 2131, 6577, 3549,\n",
       "         3431, 1318, 7670, 4729, 7149, 4534, 2645, 3456, 7525, 5071, 3070, 1404,\n",
       "         3410, 3949, 5163, 2390, 4154, 6460, 7098, 7818, 2706, 5812, 7903, 5782,\n",
       "          530, 2238,  291, 2219, 3717, 3410, 4328, 5163, 6670, 2634, 3785, 6468,\n",
       "         6595, 2494, 5209, 5899, 7831, 1287, 6843,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "           68,  437,   95, 2859]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svg_token_ids[:compress_padding_mask.sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4769, 6022, 2332, 7758, 1569, 6788, 5531, 7700, 6174, 4948, 7938, 4768,\n",
       "         7525, 4729, 7938, 7561, 3135, 4246, 4114, 8102, 3024, 5743, 5396, 5017,\n",
       "         5853, 1010, 3793, 4587, 7054, 3964, 4413, 3545,  507, 2131, 6577, 3549,\n",
       "         3431, 1318, 7670, 4729, 7149, 4534, 2645, 3456, 7525, 5071, 3070, 1404,\n",
       "         3410, 3949, 5163, 2390, 4154, 6460, 7098, 7818, 2706, 5812, 7903, 5782,\n",
       "          530, 2238,  291, 2219, 3717, 3410, 4328, 5163, 6670, 2634, 3785, 6468,\n",
       "         6595, 2494, 5209, 5899, 7831, 1287, 6843,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "          728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,  728,\n",
       "           68,  437,   95, 2859]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svg_token_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
