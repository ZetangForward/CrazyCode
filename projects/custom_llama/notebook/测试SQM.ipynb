{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import transformers\n",
    "import sys\n",
    "sys.path.append(\"/workspace/zecheng/modelzipper/projects/custom_llama\")\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import Trainer\n",
    "from modelzipper.tutils import *\n",
    "from data.vqseq2seq_dataset import OfflineBasicDataset\n",
    "from models.vqvae import VQVAE, postprocess\n",
    "from data.svg_data import *\n",
    "import pytorch_lightning as pl\n",
    "from utils.visualize_svg import convert_svg\n",
    "import transformers\n",
    "from tqdm import trange\n",
    "from PIL import Image\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh.pkl\"\n",
    "\n",
    "VQVAE_CONFIG_PATH = \"/workspace/zecheng/modelzipper/projects/custom_llama/configs/deepspeed/vqvae_config.yaml\"\n",
    "DATA_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh.pkl\"\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"/zecheng2/model_hub/flan-t5-xl\")\n",
    "\n",
    "content = auto_read_data(DATA_PATH)\n",
    "dataset = OfflineBasicDataset(content=content, tokenizer=tokenizer, mode='test')\n",
    "vqvae_config = load_yaml_config(VQVAE_CONFIG_PATH)\n",
    "\n",
    "block_kwargs = dict(\n",
    "        width=vqvae_config.vqvae_conv_block.width, \n",
    "        depth=vqvae_config.vqvae_conv_block.depth, \n",
    "        m_conv=vqvae_config.vqvae_conv_block.m_conv,\n",
    "        dilation_growth_rate=vqvae_config.vqvae_conv_block.dilation_growth_rate,\n",
    "        dilation_cycle=vqvae_config.vqvae_conv_block.dilation_cycle,\n",
    "        reverse_decoder_dilation=vqvae_config.vqvae_conv_block.vqvae_reverse_decoder_dilation\n",
    "    )\n",
    "\n",
    "def add_background(image_obj=None, save_suffix=\"b\", raw_image_size_w=None, raw_image_size_h=None):\n",
    "    image = image_obj\n",
    "   \n",
    "    sub_image_w = raw_image_size_w if raw_image_size_w is not None else image.size[0]\n",
    "    sub_image_h = raw_image_size_h if raw_image_size_h is not None else image.size[1]\n",
    "\n",
    "    new_image_size = (sub_image_w, sub_image_h)\n",
    "    background_image = Image.new('RGB', new_image_size)\n",
    "\n",
    "    background_image.paste(image, (0, 0))\n",
    "\n",
    "    return background_image\n",
    "\n",
    "class PluginVQVAE(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "vqvae = VQVAE(vqvae_config, multipliers=None, **block_kwargs)\n",
    "plugin_vqvae = PluginVQVAE(vqvae)\n",
    "checkpoint = torch.load(vqvae_config.ckpt_path)  # load vqvae ckpt\n",
    "plugin_vqvae.load_state_dict(checkpoint['state_dict'])\n",
    "plugin_vqvae.eval()\n",
    "plugin_vqvae.cuda()\n",
    "plugin_vqvae.model.half()\n",
    "\n",
    "vq_test = []\n",
    "for i in trange(len(dataset)):\n",
    "    \n",
    "    keys = tokenizer.decode(dataset[i]['text_input_ids'], skip_special_tokens=True)\n",
    "    cur_save_case = {\"keys\": keys}\n",
    "    zs = dataset[i]['svg_tensors'][1:]\n",
    "    cur_save_case['zs_len'] = len(zs)\n",
    "    with torch.no_grad():\n",
    "        PI_RES = plugin_vqvae.model.decode(zs.unsqueeze(0).cuda(), 0, 1, padding_mask=None, path_interpolation=True, return_postprocess=True)[0]\n",
    "        PC_RES = plugin_vqvae.model.decode(zs.unsqueeze(0).cuda(), 0, 1, padding_mask=None, path_interpolation=False, return_postprocess=True)[0]\n",
    "        \n",
    "        cur_save_case['pi_res_len'] = PI_RES.size(0)\n",
    "        cur_save_case['pc_res_len'] = PC_RES.size(0)\n",
    "        cur_save_case['gt_res_len'] = dataset[i]['mesh_data'].size(0)\n",
    "        \n",
    "        PI_RES_image, PI_RES_str = convert_svg(PI_RES, True)\n",
    "        PC_RES_image, PC_RES_str = convert_svg(PC_RES, True)\n",
    "        GOLDEN_image, GT_str = convert_svg(dataset[i]['mesh_data'], True)\n",
    "        \n",
    "        cur_save_case['pi_res_str'] = PI_RES_image.numericalize(n=200).to_str()\n",
    "        cur_save_case['pc_res_str'] = PC_RES_image.numericalize(n=200).to_str()\n",
    "        cur_save_case['gt_str'] = GOLDEN_image.numericalize(n=200).to_str()\n",
    "        \n",
    "        PI_RES_IMAGE_PATH = os.path.join(\"/zecheng2/evaluation/test_vq/version_8/image\", f\"PI_{i}.png\")\n",
    "        PC_RES_IMAGE_PATH = os.path.join(\"/zecheng2/evaluation/test_vq/version_8/image\", f\"PC_{i}.png\")\n",
    "        GT_IMAGE_PATH = os.path.join(\"/zecheng2/evaluation/test_vq/version_8/image\", f\"GT_{i}.png\")\n",
    "        \n",
    "        # PI_RES_image_b = add_background(PI_RES_image)\n",
    "        # PC_RES_image_b = add_background(PC_RES_image)\n",
    "        # GT_RES_image_b = add_background(GT_IMAGE_PATH)\n",
    "        \n",
    "        PI_RES_image.save_png(PI_RES_IMAGE_PATH)\n",
    "        PC_RES_image.save_png(PC_RES_IMAGE_PATH)\n",
    "        GOLDEN_image.save_png(GT_IMAGE_PATH)\n",
    "        \n",
    "        cur_save_case['PI_RES_image_path'] = PI_RES_IMAGE_PATH\n",
    "        cur_save_case['PC_RES_image_path'] = PC_RES_IMAGE_PATH\n",
    "        cur_save_case['GT_image_path'] = GT_IMAGE_PATH\n",
    "        \n",
    "        vq_test.append(cur_save_case)\n",
    "    \n",
    "auto_save_data(vq_test, \"/zecheng2/evaluation/test_vq/version_8/vq_test.pkl\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
