{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the full data for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data from /zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/inference_full_data_compress_1_snaps_merged.pkl ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['keys', 'zs'])\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/inference_full_data_compress_1_snaps_merged.pkl\"\n",
    "content = auto_read_data(FILE_PATH)\n",
    "\n",
    "length_ = len(content)\n",
    "print(content[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"/zecheng2/svg/icon-shop/pkl_data/full_data_snaps_24\"\n",
    "\n",
    "NUM_OF_SPLITS = 24\n",
    "\n",
    "sub_length = length_ // NUM_OF_SPLITS + 1\n",
    "\n",
    "for i in range(NUM_OF_SPLITS):\n",
    "    start = i * sub_length\n",
    "    end = (i + 1) * sub_length\n",
    "    if end > length_:\n",
    "        end = length_\n",
    "    print_c(f\"begin {i}th split, start: {start}, end: {end}\", \"green\")\n",
    "    sub_content = content[start:end]\n",
    "    save_path = os.path.join(SAVE_DIR, \"sub_full_data_{}.pkl\".format(i))\n",
    "    auto_save_data(sub_content, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Padding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl\"\n",
    "\n",
    "test_data = auto_read_data(FILE_PATH)\n",
    "\n",
    "print(test_data[0])\n",
    "\n",
    "print(test_data[0].keys())\n",
    "\n",
    "fake_tensor = torch.empty(4, 9).fill_(0)\n",
    "\n",
    "tmp = {\n",
    "    'keywords': ['svg_padding'],\n",
    "    'mesh_data': fake_tensor,\n",
    "}\n",
    "\n",
    "auto_save_data(tmp, \"/zecheng2/svg/icon-shop/test_data_snaps/pad_fake_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Testing Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelZipper is ready for launchðŸš€ | Current VersionðŸ¦„ >>> 0.2.6 <<< | AOE TimeðŸ•’ 2024-01-12 17:41:04\n",
      "begin to read data from /zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl ...\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl\"\n",
    "\n",
    "test_data = auto_read_data(FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Testing Data with More Instruction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1449774/1449774 [00:02<00:00, 619041.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data_with_5_kwds: 854385\n",
      "length of data_with_10_kwds: 127490\n",
      "length of data_with_15_kwds: 17242\n",
      "length of data_with_20_kwds: 9038\n",
      "length of data_with_25_kwds: 4227\n",
      "length of data_with_30_kwds: 4623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1449774/1449774 [00:07<00:00, 187016.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data_with_50_paths: 346731\n",
      "length of data_with_100_paths: 66687\n",
      "length of data_with_150_paths: 20202\n",
      "length of data_with_200_paths: 8329\n",
      "length of data_with_250_paths: 13294\n",
      "length of data_with_300_paths: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_5_kwds = []\n",
    "data_with_10_kwds = []\n",
    "data_with_15_kwds = []\n",
    "data_with_20_kwds = []\n",
    "data_with_25_kwds = []\n",
    "data_with_30_kwds = []\n",
    "\n",
    "with tqdm(total=len(content)) as pbar:\n",
    "    for i, sample in enumerate(content):\n",
    "        if len(sample['keys']) >= 30:\n",
    "            data_with_30_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 25:\n",
    "            data_with_25_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 20:\n",
    "            data_with_20_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 15:\n",
    "            data_with_15_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 10:\n",
    "            data_with_10_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 5:\n",
    "            data_with_5_kwds.append(sample)\n",
    "        pbar.update(1)\n",
    "        \n",
    "print(f\"length of data_with_5_kwds: {len(data_with_5_kwds)}\")\n",
    "print(f\"length of data_with_10_kwds: {len(data_with_10_kwds)}\")\n",
    "print(f\"length of data_with_15_kwds: {len(data_with_15_kwds)}\")\n",
    "print(f\"length of data_with_20_kwds: {len(data_with_20_kwds)}\")\n",
    "print(f\"length of data_with_25_kwds: {len(data_with_25_kwds)}\")\n",
    "print(f\"length of data_with_30_kwds: {len(data_with_30_kwds)}\")\n",
    "\n",
    "data_with_50_paths = []\n",
    "data_with_100_paths = []\n",
    "data_with_150_paths = []\n",
    "data_with_200_paths = []\n",
    "data_with_250_paths = []\n",
    "data_with_300_paths = []\n",
    "\n",
    "with tqdm(total=len(content)) as pbar:\n",
    "    for i, sample in enumerate(content):\n",
    "        if len(sample['zs']) >= 300:\n",
    "            data_with_300_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 250:\n",
    "            data_with_250_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 200:\n",
    "            data_with_200_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 150:\n",
    "            data_with_150_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 100:\n",
    "            data_with_100_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 50:\n",
    "            data_with_50_paths.append(sample)\n",
    "        pbar.update(1)\n",
    "        \n",
    "print(f\"length of data_with_50_paths: {len(data_with_50_paths)}\")\n",
    "print(f\"length of data_with_100_paths: {len(data_with_100_paths)}\")\n",
    "print(f\"length of data_with_150_paths: {len(data_with_150_paths)}\")\n",
    "print(f\"length of data_with_200_paths: {len(data_with_200_paths)}\")\n",
    "print(f\"length of data_with_250_paths: {len(data_with_250_paths)}\")\n",
    "print(f\"length of data_with_300_paths: {len(data_with_300_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257019\n",
      "50622\n",
      "15587\n",
      "6564\n",
      "10679\n",
      "237381\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/augment_stage2_data.pkl | len: 237381\n"
     ]
    }
   ],
   "source": [
    "## filter data with 5 keywords\n",
    "\n",
    "def remove_duplicates(dicts, unique_key):  \n",
    "    seen = set()  \n",
    "    unique_dicts = []  \n",
    "    for d in dicts:  \n",
    "        identifier = tuple(d[unique_key]) \n",
    "        if identifier not in seen:  \n",
    "            seen.add(identifier)  \n",
    "            unique_dicts.append(d)  \n",
    "    return unique_dicts\n",
    "\n",
    "data_with_50_paths = list(filter(lambda x: len(x['keys']) >= 5, data_with_50_paths))\n",
    "data_with_100_paths = list(filter(lambda x: len(x['keys']) >= 5, data_with_100_paths))\n",
    "data_with_150_paths = list(filter(lambda x: len(x['keys']) >= 5, data_with_150_paths))\n",
    "data_with_200_paths = list(filter(lambda x: len(x['keys']) >= 5, data_with_200_paths))\n",
    "data_with_250_paths = list(filter(lambda x: len(x['keys']) >= 5, data_with_250_paths))\n",
    "\n",
    "print(len(data_with_50_paths))\n",
    "print(len(data_with_100_paths))\n",
    "print(len(data_with_150_paths))\n",
    "print(len(data_with_200_paths))\n",
    "print(len(data_with_250_paths))\n",
    "\n",
    "\n",
    "Instruction_data = data_with_15_kwds + data_with_20_kwds + data_with_25_kwds + data_with_30_kwds\n",
    "Long_svg_gen_data = data_with_50_paths + data_with_100_paths + data_with_150_paths + data_with_200_paths + data_with_250_paths\n",
    "Combine_data = Instruction_data + Long_svg_gen_data\n",
    "\n",
    "unique_key = 'keys' \n",
    "Combine_data = remove_duplicates(Combine_data, unique_key)  \n",
    "\n",
    "print(len(Combine_data))\n",
    "\n",
    "auto_save_data(Combine_data, \"/zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/augment_stage2_data.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample Data with more than 200 Path and Keywords >= 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "begin to read data from /zecheng2/svg/icon-shop/pkl_data/full_data.pkl ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh.pkl | len: 2000\n"
     ]
    }
   ],
   "source": [
    "sample1 = random.sample(data_with_50_paths, k=500)\n",
    "sample2 = random.sample(data_with_200_paths, k=500)\n",
    "sample3 = random.sample(data_with_150_paths, k=500)\n",
    "sample4 = random.sample(data_with_100_paths, k=500)\n",
    "\n",
    "test_content = sample1 + sample2 + sample3 + sample4\n",
    "\n",
    "min_kwd = 10000\n",
    "for i, sample in enumerate(test_content):\n",
    "    min_kwd = min(min_kwd, len(sample['keys']))\n",
    "\n",
    "print(min_kwd)\n",
    "\n",
    "keywords_dict = {}\n",
    "\n",
    "tmp = ()\n",
    "\n",
    "\n",
    "raw_data = auto_read_data(\"/zecheng2/svg/icon-shop/pkl_data/full_data.pkl\")\n",
    "\n",
    "for i, item in enumerate(raw_data):\n",
    "    keywords = item['keywords']\n",
    "    keywords_dict[tuple(keywords)] = i\n",
    "\n",
    "# test_content = auto_read_data(\"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq.pkl\")\n",
    "cnt = 0\n",
    "for sample in test_content:    \n",
    "    keywords = tuple(sample['keys'])\n",
    "    if cnt < 500:\n",
    "        sample['level'] = 'short'\n",
    "    elif cnt < 1000:\n",
    "        sample['level'] = 'mid'\n",
    "    elif cnt < 1500:\n",
    "        sample['level'] = 'long'\n",
    "    else:\n",
    "        sample['level'] = 'extreme long'\n",
    "        \n",
    "    if keywords in keywords_dict:\n",
    "        sample['mesh_data'] = raw_data[keywords_dict[keywords]]['mesh_data']\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "auto_save_data(test_content, \"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh.pkl\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/stage2_training_data.pkl | len: 162620\n"
     ]
    }
   ],
   "source": [
    "stage2_training_data = data_with_10_kwds + data_with_15_kwds + data_with_20_kwds + data_with_25_kwds + data_with_30_kwds\n",
    "auto_save_data(stage2_training_data, \"/zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/stage2_training_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/test_15_kwds_2000_samples.pkl | len: 2000\n"
     ]
    }
   ],
   "source": [
    "tesing_set = data_with_15_kwds[:2000]\n",
    "auto_save_data(tesing_set, \"/zecheng2/svg/icon-shop/test_data_snaps/test_15_kwds_2000_samples.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Testing data for 8 snaps for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelZipper is ready for launchðŸš€ | Current VersionðŸ¦„ >>> 0.2.6 <<< | AOE TimeðŸ•’ 2024-01-21 01:17:50\n",
      "/zecheng2/svg/icon-shop/test_data_snaps/split_snaps not exist! --> Create dir /zecheng2/svg/icon-shop/test_data_snaps/split_snaps\n",
      "begin to read data from /zecheng2/svg/icon-shop/test_data_snaps/test_data_long_seq_with_mesh.pkl ...\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_0.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_1.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_2.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_3.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_4.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_5.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_6.pkl | len: 251\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps/long_test_split_7.pkl | len: 243\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FULL_TESTING_DATA = \"/zecheng2/svg/icon-shop/test_data_snaps/test_data_long_seq_with_mesh.pkl\"\n",
    "SPLIT_DATA_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/split_snaps\"\n",
    "auto_mkdir(SPLIT_DATA_PATH)\n",
    "\n",
    "SPLIT_NUM = 8\n",
    "\n",
    "content = auto_read_data(FULL_TESTING_DATA)\n",
    "\n",
    "PER_SPLIT_NUM = len(content) // SPLIT_NUM + 1\n",
    "\n",
    "for i in range(SPLIT_NUM):\n",
    "    start = i * PER_SPLIT_NUM\n",
    "    end = (i + 1) * PER_SPLIT_NUM\n",
    "    if end > len(content):\n",
    "        end = len(content)\n",
    "    tmp = content[start:end]\n",
    "    auto_save_data(tmp, os.path.join(SPLIT_DATA_PATH, f\"long_test_split_{i}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_mask': tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True]),\n",
       " 'keys': ['Traveler',\n",
       "  'programmer',\n",
       "  'customer',\n",
       "  'support',\n",
       "  'Meeting',\n",
       "  'adults',\n",
       "  'Partner',\n",
       "  'Family',\n",
       "  'users',\n",
       "  'Man',\n",
       "  'Human',\n",
       "  'politician',\n",
       "  'Employee',\n",
       "  'Woman',\n",
       "  'teamwork',\n",
       "  'manager',\n",
       "  'administrator',\n",
       "  'Designer',\n",
       "  'Worker',\n",
       "  'women',\n",
       "  'advocate',\n",
       "  'businessmen',\n",
       "  'admin',\n",
       "  'Society',\n",
       "  'Social',\n",
       "  'engineer',\n",
       "  'User',\n",
       "  'president',\n",
       "  'account',\n",
       "  'group',\n",
       "  'people',\n",
       "  'friends',\n",
       "  'team',\n",
       "  'men',\n",
       "  'management',\n",
       "  'Community',\n",
       "  'developer',\n",
       "  'officer',\n",
       "  'Businessman',\n",
       "  'Businesswoman',\n",
       "  'Network',\n",
       "  'boy',\n",
       "  'supervisor',\n",
       "  'person',\n",
       "  'Employer',\n",
       "  'Architect',\n",
       "  'profile',\n",
       "  'businesswomen',\n",
       "  'HR',\n",
       "  'Human Resource',\n",
       "  'leader',\n",
       "  'reader',\n",
       "  'reporter',\n",
       "  'Caucasians'],\n",
       " 'zs': tensor([4554, 3962, 2466, 8032, 3296, 3259, 7026, 3378, 2818, 6732,  533, 7807,\n",
       "         3723, 7684, 3237, 2340, 3266, 2300, 7385, 5626, 6348, 2953, 5737, 3961,\n",
       "         3410, 4894, 5440, 7374, 7158, 7652, 4252, 7932, 7974, 4524, 3777, 3930,\n",
       "         2856, 1614, 6573, 1776,  643,  899, 1327, 7553, 6838, 4810, 2707, 1529])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_30_kwds[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
