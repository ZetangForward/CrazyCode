{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the full data for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.6 <<< | AOE Time🕒 2024-01-22 03:31:25\n",
      "begin to read data from /zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/inference_full_data_compress_1_snaps_merged.pkl ...\n",
      "dict_keys(['keys', 'zs'])\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/inference_full_data_compress_1_snaps_merged.pkl\"\n",
    "content = auto_read_data(FILE_PATH)\n",
    "\n",
    "length_ = len(content)\n",
    "print(content[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清理训练数据集，删除里面过长的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data from /zecheng2/svg/icon-shop/pkl_data/full_data.pkl ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['keywords', 'mesh_data', 'category_name'])\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/pkl_data/full_data.pkl\"\n",
    "content = auto_read_data(FILE_PATH)\n",
    "\n",
    "length_ = len(content)\n",
    "print(content[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"/zecheng2/svg/icon-shop/pkl_data/full_data_snaps_24\"\n",
    "\n",
    "NUM_OF_SPLITS = 24\n",
    "\n",
    "sub_length = length_ // NUM_OF_SPLITS + 1\n",
    "\n",
    "for i in range(NUM_OF_SPLITS):\n",
    "    start = i * sub_length\n",
    "    end = (i + 1) * sub_length\n",
    "    if end > length_:\n",
    "        end = length_\n",
    "    print_c(f\"begin {i}th split, start: {start}, end: {end}\", \"green\")\n",
    "    sub_content = content[start:end]\n",
    "    save_path = os.path.join(SAVE_DIR, \"sub_full_data_{}.pkl\".format(i))\n",
    "    auto_save_data(sub_content, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Padding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl\"\n",
    "\n",
    "test_data = auto_read_data(FILE_PATH)\n",
    "\n",
    "print(test_data[0])\n",
    "\n",
    "print(test_data[0].keys())\n",
    "\n",
    "fake_tensor = torch.empty(4, 9).fill_(0)\n",
    "\n",
    "tmp = {\n",
    "    'keywords': ['svg_padding'],\n",
    "    'mesh_data': fake_tensor,\n",
    "}\n",
    "\n",
    "auto_save_data(tmp, \"/zecheng2/svg/icon-shop/test_data_snaps/pad_fake_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Testing Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelZipper is ready for launch🚀 | Current Version🦄 >>> 0.2.6 <<< | AOE Time🕒 2024-01-12 17:41:04\n",
      "begin to read data from /zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl ...\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FILE_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/test_mesh_data_svg_convert_p.pkl\"\n",
    "\n",
    "test_data = auto_read_data(FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Testing Data with More Instruction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 139252/1449774 [00:00<00:01, 669682.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449774/1449774 [00:01<00:00, 734045.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data_with_5_kwds: 854385\n",
      "length of data_with_10_kwds: 127490\n",
      "length of data_with_15_kwds: 17242\n",
      "length of data_with_20_kwds: 9038\n",
      "length of data_with_25_kwds: 4227\n",
      "length of data_with_30_kwds: 4623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1449774/1449774 [00:07<00:00, 190177.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of data_with_50_paths: 346731\n",
      "length of data_with_100_paths: 66687\n",
      "length of data_with_150_paths: 20202\n",
      "length of data_with_200_paths: 8329\n",
      "length of data_with_250_paths: 13294\n",
      "length of data_with_300_paths: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_5_kwds = []\n",
    "data_with_10_kwds = []\n",
    "data_with_15_kwds = []\n",
    "data_with_20_kwds = []\n",
    "data_with_25_kwds = []\n",
    "data_with_30_kwds = []\n",
    "\n",
    "with tqdm(total=len(content)) as pbar:\n",
    "    for i, sample in enumerate(content):\n",
    "        if len(sample['keys']) >= 30:\n",
    "            data_with_30_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 25:\n",
    "            data_with_25_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 20:\n",
    "            data_with_20_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 15:\n",
    "            data_with_15_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 10:\n",
    "            data_with_10_kwds.append(sample)\n",
    "        elif len(sample['keys']) >= 5:\n",
    "            data_with_5_kwds.append(sample)\n",
    "        pbar.update(1)\n",
    "        \n",
    "print(f\"length of data_with_5_kwds: {len(data_with_5_kwds)}\")\n",
    "print(f\"length of data_with_10_kwds: {len(data_with_10_kwds)}\")\n",
    "print(f\"length of data_with_15_kwds: {len(data_with_15_kwds)}\")\n",
    "print(f\"length of data_with_20_kwds: {len(data_with_20_kwds)}\")\n",
    "print(f\"length of data_with_25_kwds: {len(data_with_25_kwds)}\")\n",
    "print(f\"length of data_with_30_kwds: {len(data_with_30_kwds)}\")\n",
    "\n",
    "data_with_50_paths = []\n",
    "data_with_100_paths = []\n",
    "data_with_150_paths = []\n",
    "data_with_200_paths = []\n",
    "data_with_250_paths = []\n",
    "data_with_300_paths = []\n",
    "\n",
    "with tqdm(total=len(content)) as pbar:\n",
    "    for i, sample in enumerate(content):\n",
    "        if len(sample['zs']) >= 300:\n",
    "            data_with_300_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 250:\n",
    "            data_with_250_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 200:\n",
    "            data_with_200_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 150:\n",
    "            data_with_150_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 100:\n",
    "            data_with_100_paths.append(sample)\n",
    "        elif len(sample['zs']) >= 50:\n",
    "            data_with_50_paths.append(sample)\n",
    "        pbar.update(1)\n",
    "        \n",
    "print(f\"length of data_with_50_paths: {len(data_with_50_paths)}\")\n",
    "print(f\"length of data_with_100_paths: {len(data_with_100_paths)}\")\n",
    "print(f\"length of data_with_150_paths: {len(data_with_150_paths)}\")\n",
    "print(f\"length of data_with_200_paths: {len(data_with_200_paths)}\")\n",
    "print(f\"length of data_with_250_paths: {len(data_with_250_paths)}\")\n",
    "print(f\"length of data_with_300_paths: {len(data_with_300_paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296014\n",
      "58259\n",
      "17742\n",
      "262221\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/augment_stage2_data_pro.pkl | len: 262221\n"
     ]
    }
   ],
   "source": [
    "## filter data with 5 keywords\n",
    "\n",
    "def remove_duplicates(dicts, unique_key):  \n",
    "    seen = set()  \n",
    "    unique_dicts = []  \n",
    "    for d in dicts:  \n",
    "        identifier = tuple(d[unique_key]) \n",
    "        if identifier not in seen:  \n",
    "            seen.add(identifier)  \n",
    "            unique_dicts.append(d)  \n",
    "    return unique_dicts\n",
    "\n",
    "data_with_50_paths = list(filter(lambda x: len(x['keys']) >= 3, data_with_50_paths))\n",
    "data_with_100_paths = list(filter(lambda x: len(x['keys']) >= 3, data_with_100_paths))\n",
    "data_with_150_paths = list(filter(lambda x: len(x['keys']) >= 3, data_with_150_paths))\n",
    "# data_with_200_paths = list(filter(lambda x: len(x['keys']) >= 3, data_with_200_paths))\n",
    "# data_with_250_paths = list(filter(lambda x: len(x['keys']) >= 3, data_with_250_paths))\n",
    "\n",
    "print(len(data_with_50_paths))\n",
    "print(len(data_with_100_paths))\n",
    "print(len(data_with_150_paths))\n",
    "# print(len(data_with_200_paths))\n",
    "# print(len(data_with_250_paths))\n",
    "\n",
    "\n",
    "Instruction_data = data_with_15_kwds + data_with_20_kwds + data_with_25_kwds + data_with_30_kwds\n",
    "Long_svg_gen_data = data_with_50_paths + data_with_100_paths + data_with_150_paths\n",
    "Combine_data = Instruction_data + Long_svg_gen_data\n",
    "\n",
    "unique_key = 'keys' \n",
    "Combine_data = remove_duplicates(Combine_data, unique_key)  \n",
    "\n",
    "print(len(Combine_data))\n",
    "\n",
    "auto_save_data(Combine_data, \"/zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/augment_stage2_data_pro.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample Data with more than 200 Path and Keywords >= 10 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "begin to read data from /zecheng2/svg/icon-shop/pkl_data/full_data.pkl ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh_visual.pkl | len: 6000\n"
     ]
    }
   ],
   "source": [
    "sample1 = random.sample(data_with_50_paths, k=2000)\n",
    "sample3 = random.sample(data_with_150_paths, k=2000)\n",
    "sample4 = random.sample(data_with_100_paths, k=2000)\n",
    "\n",
    "test_content = sample1 + sample3 + sample4\n",
    "\n",
    "min_kwd = 10000\n",
    "for i, sample in enumerate(test_content):\n",
    "    min_kwd = min(min_kwd, len(sample['keys']))\n",
    "\n",
    "print(min_kwd)\n",
    "\n",
    "keywords_dict = {}\n",
    "\n",
    "tmp = ()\n",
    "\n",
    "\n",
    "raw_data = auto_read_data(\"/zecheng2/svg/icon-shop/pkl_data/full_data.pkl\")\n",
    "\n",
    "for i, item in enumerate(raw_data):\n",
    "    keywords = item['keywords']\n",
    "    keywords_dict[tuple(keywords)] = i\n",
    "\n",
    "# test_content = auto_read_data(\"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq.pkl\")\n",
    "cnt = 0\n",
    "for sample in test_content:    \n",
    "    keywords = tuple(sample['keys'])\n",
    "    if cnt < 500:\n",
    "        sample['level'] = 'short'\n",
    "    elif cnt < 1000:\n",
    "        sample['level'] = 'mid'\n",
    "    elif cnt < 1500:\n",
    "        sample['level'] = 'long'\n",
    "    else:\n",
    "        sample['level'] = 'extreme long'\n",
    "        \n",
    "    if keywords in keywords_dict:\n",
    "        sample['mesh_data'] = raw_data[keywords_dict[keywords]]['mesh_data']\n",
    "        cnt += 1\n",
    "\n",
    "print(cnt)\n",
    "\n",
    "auto_save_data(test_content, \"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh_visual.pkl\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/stage2_training_data.pkl | len: 162620\n"
     ]
    }
   ],
   "source": [
    "stage2_training_data = data_with_10_kwds + data_with_15_kwds + data_with_20_kwds + data_with_25_kwds + data_with_30_kwds\n",
    "auto_save_data(stage2_training_data, \"/zecheng2/svg/icon-shop/pkl_data/efficient_inference_full_data/test_vqllama_quantizer_testset/version_12/epoch_37/stage2_training_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/test_15_kwds_2000_samples.pkl | len: 2000\n"
     ]
    }
   ],
   "source": [
    "tesing_set = data_with_15_kwds[:2000]\n",
    "auto_save_data(tesing_set, \"/zecheng2/svg/icon-shop/test_data_snaps/test_15_kwds_2000_samples.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Testing data for 8 snaps for efficient inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3 not exist! --> Create dir /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3\n",
      "begin to read data from /zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh_visual.pkl ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_0.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_1.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_2.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_3.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_4.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_5.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_6.pkl | len: 751\n",
      "pkl file saved successfully!\n",
      "Save file to /zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3/long_test_split_7.pkl | len: 743\n"
     ]
    }
   ],
   "source": [
    "from modelzipper.tutils import *\n",
    "\n",
    "FULL_TESTING_DATA = \"/zecheng2/svg/icon-shop/test_data_snaps/test_data_all_seq_with_mesh_visual.pkl\"\n",
    "SPLIT_DATA_PATH = \"/zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v3\"\n",
    "auto_mkdir(SPLIT_DATA_PATH)\n",
    "\n",
    "SPLIT_NUM = 8\n",
    "\n",
    "content = auto_read_data(FULL_TESTING_DATA)\n",
    "\n",
    "PER_SPLIT_NUM = len(content) // SPLIT_NUM + 1\n",
    "\n",
    "for i in range(SPLIT_NUM):\n",
    "    start = i * PER_SPLIT_NUM\n",
    "    end = (i + 1) * PER_SPLIT_NUM\n",
    "    if end > len(content):\n",
    "        end = len(content)\n",
    "    tmp = content[start:end]\n",
    "    auto_save_data(tmp, os.path.join(SPLIT_DATA_PATH, f\"long_test_split_{i}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin to read data from /zecheng2/vqllama/test_vq_seq2seq/test_flat_t5_aug_v7/snap_6_results.pkl ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'golden_svg_path': tensor([4097, 3644, 3511, 3459,  646, 3511, 3475, 2818, 3459, 2121, 3695, 1910,\n",
      "         646, 3924, 2831, 3555, 2446,  646,  142,  115, 2511, 4003, 1250, 3144,\n",
      "        3230,  646, 3871,  933, 2208, 2879,  854, 2808, 2007, 2208, 2879, 2736,\n",
      "        3676, 3435, 3160,  380, 4021, 1358, 1634,  740, 2074,  657, 1928, 2369,\n",
      "        1673, 3608, 3114, 1375, 3770, 3784,  738,  701, 3804, 3230, 3254, 3236,\n",
      "        4054,  491, 2480, 1673, 3489, 2538, 1706,  656,  783, 3708, 2007, 1739,\n",
      "        4070, 2783, 1739, 3559, 2783, 4025, 2549, 3605, 2753, 3908, 2452,  404,\n",
      "        3216, 1256, 1062,  619, 3142, 2561, 2609, 3079, 3472, 2746, 3637, 2957,\n",
      "         577, 1401, 3532, 3318,  619,  305,  332,  769,  570,  619, 2418,  570,\n",
      "        1749, 2418, 2746,  639, 1395,   82, 3315, 1158,   82, 3878,  639, 1424,\n",
      "        2160, 1401, 2783, 1053,  925, 2736, 3160,  577, 3105, 1158, 1023, 1496,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0]), 'golden_svg_padding_mask': tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False]), 'generated_svg_path': tensor([[  0,   0,   0,  ...,   0,   6, 103],\n",
      "        [  1,   6, 103,  ...,   0,   8, 199],\n",
      "        [  1,   8, 199,  ...,   0,  16, 200],\n",
      "        ...,\n",
      "        [  2,  26, 185,  ..., 184,  29, 183],\n",
      "        [  2,  29, 183,  ..., 179,  33, 180],\n",
      "        [  2,  33, 180,  ..., 171,  33, 179]]), 'text': 'Bikini, Summer, Crab, sun, relax, holidays, Sea, beach, sunbathing, Ball', 'svg_token_ids': tensor([2622, 3440,  508, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053,\n",
      "        4053, 4053, 4053, 4053, 4053, 4053, 4053, 4053]), 'raw_data': tensor([[  0,   4, 104,  ...,   0,  86,  35],\n",
      "        [  2,  86,  35,  ...,  38,  85,  38],\n",
      "        [  2,  85,  38,  ...,  37,  83,  35],\n",
      "        ...,\n",
      "        [  0,   0,   0,  ...,   0,   0,   0],\n",
      "        [  0,   0,   0,  ...,   0,   0,   0],\n",
      "        [  0,   0,   0,  ...,   0,   0,   0]])}\n"
     ]
    }
   ],
   "source": [
    "sample = auto_read_data(\"/zecheng2/vqllama/test_vq_seq2seq/test_flat_t5_aug_v7/snap_6_results.pkl\")\n",
    "\n",
    "print(sample[0])\n",
    "# auto_save_data(re_save, \"/zecheng2/svg/icon-shop/test_data_snaps/split_snaps_v2/long_test_split_7.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
