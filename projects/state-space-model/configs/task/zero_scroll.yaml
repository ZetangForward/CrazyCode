dataset:
  data_path: "${dataset_path}/ZeroSCROLLS"
  processed_data_path: "${dataset_path}/ZeroSCROLLS/all_testing_data.pkl"
  ctx_len: 10000
  subsets: ['gov_report', 'summ_screen_fd', 'qmsum', 'qasper', 'narrative_qa', 'quality', 'musique', 'squality', 'space_digest','book_sum_sort']
  nworkers: 12
  pin_memory: False
  inference_mode: True
  cluster_batch: False

model:
  model_name_or_path: "${hf_model_path}/mamba-1.4b"
  ckpt_path: "${hf_model_path}/mamba-1.4b/pytorch_model.bin"
  load_model_state_dict: True
  use_position: False
  # ckpt_path: "/nvme/zecheng/ckpt/mamba-chat/checkpoint-1000/pytorch_model.bin"

tokenizer:
  tokenizer_name_or_path: "${hf_model_path}/EleutherAI/gpt-neox-20b"

experiment:
  seed: 27
  max_seq_length: 512
  results_save_dir: ${exp_path}/${task}/results
  test_task: ${task}
  
