SAVE_DIR: &SAVE_DIR "/public/home/ljt/tzc/ckpt"
JOB_ID: &JOB_ID 1
TASK: &TASK "mamba_context_scaling"

hydra:
  job:
    id: ${JOB_ID}
    name: hydra_${TASK}
  run:
    dir: ${SAVE_DIR}/${TASK}/version_${JOB_ID}/${hydra.job.name} 

dataset:
  file_path: "/public/home/ljt/tzc/data/LongQLoRA-Dataset/LongQLoRA-SFT-Data-39k.jsonl"
  max_seq_length: 256
  train_batch_size: 8
  val_batch_size: 8
  nworkers: 1
  pin_memory: False
  inference_mode: False
  cluster_batch: False

model:
  model_name_or_path: "/public/home/ljt/hf_models/mamba-1.4b"
  
tokenizer:
  tokenizer_name_or_path: "/public/home/ljt/hf_models/gpt-neox-20b"

optimizer:
  optimizer_type: "adamw"
  lr: 5e-5

lr_scheduler:
  scheduler_type: "get_cosine_schedule_with_warmup"
  warmup_steps: 0

experiment:
  seed: 27
  model_save_dir: ${SAVE_DIR}
  task: ${TASK}
  version: ${JOB_ID}
  num_training_steps: 10000
  device_num: 1
  node_num: 1